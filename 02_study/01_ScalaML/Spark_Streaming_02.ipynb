{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2. Architecture and Components of Spark and Spark Streaming\n",
    "\n",
    "- 하둡의 장점\n",
    "    - revolution in data processing and storage space\n",
    "    - a low cost solution and reliable batch processing\n",
    "    - http://en.wikipedia.org/wiki/MapReduce\n",
    "- 하둡MapReduce의 한계점\n",
    "    - Excessive and intensive use of disks for all intermediate stages\n",
    "    - Only provides map and reduce operations and no other operations like joining/flattening, and grouping of datasets.\n",
    "- Spark 장점 \n",
    "    - enabled in-memory data storage and near real-time data processing.\n",
    "    - operations such as joins, merging, grouping and many more\n",
    "    - faster ( disk를 사용하는 hadoop app 보다 )\n",
    "- Spark의 Core API\n",
    "    - SQL for structured data processing\n",
    "    - MLlib for iterative data processing—machine learning\n",
    "    - GraphX for graph processing\n",
    "    - Spark Streaming—real-time data processing of streaming data\n",
    "- 이번장의 목표\n",
    "    - Batch versus real-time data processing\n",
    "    - Architecture of Spark\n",
    "    - Architecture of Spark Streaming\n",
    "    - Your first Spark Streaming program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01절 Batch versus real-time data processing\n",
    "- batch 와 real-time에서 large dataset의 데이터 프로세싱의 다른점을 논의하자\n",
    "- 그러면, Spark architecture를 이해하는데 많은 도움 됨.\n",
    "\n",
    "### Batch processing\n",
    "- Batch processing는 서로 다른 JOB이 연결되어 있거나 또는 다른 JOB이후에 순서적으로 또는 병렬로 실행되는 JOB들이 여러개로 이루어진 process 임.\n",
    "- 입력데이터는 일정시간 동안 수집하고  batch의 결과가 다른 JOB의 입력이 될 수 있음.\n",
    "- fast response time은 핵심이 아님.\n",
    "- Job을 processing하기 위한 time window or batch window을 갖음.\n",
    "- 다소 덜 민감한 온라인 활동(less intensive online activity)하는 주기를 갖음.\n",
    "- examples of batch jobs\n",
    "    - Log analysis\n",
    "    - Billing applications\n",
    "    - Backups\n",
    "    - Data warehouses\n",
    "- complexity involved in batch processing systems\n",
    "    - Large data\n",
    "    - Scalability\n",
    "    - Distributed processing\n",
    "    - Fault tolerant\n",
    "    \n",
    "### Real-time data processing\n",
    "- Real-time data processing는 항공관제나 여행 예약시스템에서와 같이 끊임없이 변경되는 데이터를 수신받고, 데이터의 소스를 컨트롤할 수 있도록 적합한 속도로 이를 처리함.\n",
    "- real time에서의 응답시간은 즉시적이고, 수 밀리초이내를 기대함.\n",
    "- 데이터 수신시간과 응답시간 사이의 차이를  **latency** 라고 하며,  작을수록 좋음.\n",
    "- real time은  latency 또는 서비스 요건이 있기 때문에 near real-time(준시실간)으로 자주 언급됨.\n",
    "- examples of real-time systems\n",
    "    - Bank ATMs\n",
    "    - Real-time monitoring\n",
    "    - Real-time business intelligence\n",
    "    - Operational intelligence (OI) \n",
    "    - Point of Sale (POS) systems\n",
    "    - Assembly lines ( 조립라인 )\n",
    "- complexity involved in real-time data processing systems \n",
    "    - System responsiveness ( 시스템 민감도 ) : 지연없이 데이터를 처리해야 함.\n",
    "    - Fault-tolerant\n",
    "    - Scalable\n",
    "    - In memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02절 Architecture of Spark \n",
    "\n",
    "### Spark versus Hadoop\n",
    "- Spark는 open source cluster computing framework로 hadoop과 비슷하지만, 더 좋음.\n",
    "- 더 좋은 이유\n",
    "    - Iterative and interactive computations and workloads : 예를 들면, 중간생산물을 재사용하는 machine learning algorith과 여러개의 병렬연산이 필요한 데이터 작업시에 좋음.\n",
    "    - Real-time data processing : hadoop은 batch processing이고, real-time시에 인-메모리 프로세싱 능력이 부족함.\n",
    "- RDD (Resilient Distributed Datasets)\n",
    "    - cluster내에서 partitioned되어지고, 지연최소화를 위해서 메모리내에 캐싱되어지는 분산데이터셋의 새로운 추상화 레이어를 도입.\n",
    "    - RDD는 immutable (read-only) collection \n",
    "    \n",
    "### Layered architecture – Spark\n",
    "\n",
    "![Spark](sparkstreaming02_01.jpg)\n",
    "\n",
    "- High-level architecture of Spark\n",
    "    - Data storage layer : local filesystems, HDFS , NoSQL database like HBase, Cassandra, MongoDB, S3, Elasticsearch\n",
    "    - Resource manager APIs : YARN, Mesos, Standalone\n",
    "    - Spark Core libraries : Spark general execution engine,  in-memory distributed data processing\n",
    "    - Spark extensions/libraries \n",
    "    \n",
    "![Spark](sparkstreaming02_02.jpg)    \n",
    "\n",
    "- Interaction between the different layers of the Spark architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 03절 Architecture of Spark Streaming \n",
    "\n",
    "### What is Spark Streaming? \n",
    "- Spark Streaming은 streaming data 또는 빠르게 흐르는 데이터를 processing하는 기능을 제공하는 Spark 확장임.\n",
    "- spam filtering, intrusion detection(침입탐지), clickstream data analysis 등에 사용됨.\n",
    "- Spark가 in-memory processing이 지원되어서, spark streaming도 in-memory에서 live/streaming data 을 processing함.\n",
    "- 데이터 소스로 HDFS, Flume, Kakfa, Twitter, TCP socket을 포함함.\n",
    "\n",
    "### High-level architecture – Spark Streaming\n",
    "\n",
    "![Spark](sparkstreaming02_03.jpg)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 04절 Your first Spark Streaming program\n",
    "\n",
    "- spark streaming 에서 특정포트를 열고, 일정 주기마다 데이터를 받아들임.\n",
    "- 받은 데이터에서 개별 단어가 몇번 나오는지 카운트를 하고 그것을 화면에 보여줌.\n",
    "- 예제에서는 2가지 부분으로 나눔.\n",
    "    - Spark Streaming job: \n",
    "    - Client application\n",
    "- http://www.packtpub.com  소스 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Coding Spark Streaming jobs in Scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "package chapter.two\n",
    "\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.streaming.StreamingContext\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.storage.StorageLevel._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.streaming.dstream.DStream\n",
    "import org.apache.spark.streaming.dstream.ForEachDStream\n",
    "\n",
    "\n",
    "object ScalaFirstSreamingExample {\n",
    "  \n",
    "  def main(args:Array[String]){\n",
    "    \n",
    "    println(\"Creating Spark Configuration\")\n",
    "    //Create an Object of Spark Configuration\n",
    "    val conf = new SparkConf()\n",
    "    //Set the logical and user defined Name of this Application\n",
    "    conf.setAppName(\"My First Spark Streaming Application\")\n",
    "    \n",
    "    println(\"Retreiving Streaming Context from Spark Conf\")\n",
    "    //Retrieving Streaming Context from SparkConf Object.\n",
    "    //Second parameter is the time interval at which streaming data will be divided into batches  \n",
    "    val streamCtx = new StreamingContext(conf, Seconds(2))\n",
    "\n",
    "    //Define the the type of Stream. Here we are using TCP Socket as text stream, \n",
    "    //It will keep watching for the incoming data from a specific machine (localhost) and port (9087) \n",
    "    //Once the data is retrieved it will be saved in the memory and in case memory\n",
    "    //is not sufficient, then it will store it on the Disk\n",
    "    //It will further read the Data and convert it into DStream\n",
    "    val lines = streamCtx.socketTextStream(\"localhost\", 9087, MEMORY_AND_DISK_SER_2)\n",
    "    \n",
    "    //Apply the Split() function to all elements of DStream \n",
    "    //which will further generate multiple new records from each record in Source Stream\n",
    "    //And then use flatmap to consolidate all records and create a new DStream.\n",
    "    val words = lines.flatMap(x => x.split(\" \"))\n",
    "    \n",
    "    //Now, we will count these words by applying a using map()\n",
    "    //map() helps in applying a given function to each element in an RDD. \n",
    "    val pairs = words.map(word => (word, 1))\n",
    "    \n",
    "    //Further we will aggregate the value of each key by using/ applying the given function.\n",
    "    val wordCounts = pairs.reduceByKey(_ + _)\n",
    "    \n",
    "    //Lastly we will print all Values\n",
    "    //wordCounts.print(20)\n",
    "    \n",
    "    printValues(wordCounts,streamCtx)\n",
    "    //Most important statement which will initiate the Streaming Context\n",
    "    streamCtx.start();\n",
    "    //Wait till the execution is completed.\n",
    "    streamCtx.awaitTermination();  \n",
    "  \n",
    "  }\n",
    "  \n",
    "  /**\n",
    "   * Simple Print function, for printing all elements of RDD\n",
    "   */\n",
    "  def printValues(stream:DStream[(String,Int)],streamCtx: StreamingContext){\n",
    "    stream.foreachRDD(foreachFunc)\n",
    "    def foreachFunc = (rdd: RDD[(String,Int)]) => {\n",
    "      val array = rdd.collect()\n",
    "      println(\"---------Start Printing Results----------\")\n",
    "      for(res<-array){\n",
    "        println(res)\n",
    "      }\n",
    "      println(\"---------Finished Printing Results----------\")\n",
    "    }\n",
    "  }\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Spark Streaming jobs in Java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "package chapter.two;\n",
    "\n",
    "import java.util.Arrays;\n",
    "\n",
    "import org.apache.spark.*;\n",
    "import org.apache.spark.api.java.function.*;\n",
    "import org.apache.spark.storage.StorageLevel;\n",
    "import org.apache.spark.streaming.*;\n",
    "import org.apache.spark.streaming.api.java.*;\n",
    "\n",
    "import scala.Tuple2;\n",
    "\n",
    "public class JavaFirstStreamingExample {\n",
    "\t  \n",
    "\tpublic static void main(String[] s){\n",
    "\t    \n",
    "\t    System.out.println(\"Creating Spark Configuration\");\n",
    "\t    //Create an Object of Spark Configuration\n",
    "\t    SparkConf conf = new SparkConf();\n",
    "\t    //Set the logical and user defined Name of this Application\n",
    "\t    conf.setAppName(\"My First Spark Streaming Application\");\n",
    "\t    //Define the URL of the Spark Master. \n",
    "\t    //Useful only if you are executing Scala App directly from the console.\n",
    "\t    //We will comment it for now but will use later\n",
    "\t    //conf.setMaster(\"spark://ip-10-237-224-94:7077\")\n",
    "\t    \n",
    "\t    System.out.println(\"Retreiving Streaming Context from Spark Conf\");\n",
    "\t    //Retrieving Streaming Context from SparkConf Object.\n",
    "\t    //Second parameter is the time interval at which streaming data will be divided into batches  \n",
    "\t    JavaStreamingContext streamCtx = new JavaStreamingContext(conf, Durations.seconds(2));\n",
    "\n",
    "\t    //Define the the type of Stream. Here we are using TCP Socket as text stream, \n",
    "\t    //It will keep watching for the incoming data from a specific machine (localhost) and port (9087) \n",
    "\t    //Once the data is retrieved it will be saved in the memory and in case memory\n",
    "\t    //is not sufficient, then it will store it on the Disk.  \n",
    "\t    //It will further read the Data and convert it into DStream\n",
    "\t    JavaReceiverInputDStream<String> lines = streamCtx.socketTextStream(\"localhost\", 9087,StorageLevel.MEMORY_AND_DISK_SER_2());\n",
    "\t    \n",
    "\t    //Apply the x.split() function to all elements of JavaReceiverInputDStream \n",
    "\t    //which will further generate multiple new records from each record in Source Stream\n",
    "\t    //And then use flatmap to consolidate all records and create a new JavaDStream.\n",
    "\t    JavaDStream<String> words = lines.flatMap( new FlatMapFunction<String, String>() {\n",
    "\t    \t\t\t    @Override public Iterable<String> call(String x) {\n",
    "\t    \t\t\t      return Arrays.asList(x.split(\" \"));\n",
    "\t    \t\t\t    }\n",
    "\t    \t\t\t  });\n",
    "\t    \t\t\n",
    "\t    \n",
    "\t    //Now, we will count these words by applying a using mapToPair()\n",
    "\t    //mapToPair() helps in applying a given function to each element in an RDD\n",
    "\t    //And further will return the Scala Tuple with \"word\" as key and value as \"count\".\n",
    "\t    JavaPairDStream<String, Integer> pairs = words.mapToPair(\n",
    "\t    \t\t  new PairFunction<String, String, Integer>() {\n",
    "\t    \t\t    @Override \n",
    "\t    \t\t    public Tuple2<String, Integer> call(String s) throws Exception {\n",
    "\t    \t\t      return new Tuple2<String, Integer>(s, 1);\n",
    "\t    \t\t    }\n",
    "\t    \t\t  });\n",
    "\t    \t\t\n",
    "\t    \n",
    "\t    //Further we will aggregate the value of each key by using/ applying the given function.\n",
    "\t    JavaPairDStream<String, Integer> wordCounts = pairs.reduceByKey(\n",
    "\t    \t\t  new Function2<Integer, Integer, Integer>() {\n",
    "\t    \t\t    @Override public Integer call(Integer i1, Integer i2) throws Exception {\n",
    "\t    \t\t      return i1 + i2;\n",
    "\t    \t\t    }\n",
    "\t    \t\t  });\n",
    "\t    \t\t\n",
    "\t    \n",
    "\t    //Lastly we will print First 10 Words.\n",
    "\t    //We can also implement custom print method for printing all values,\n",
    "\t    //as we did in Scala example.\n",
    "\t    wordCounts.print(10);\n",
    "\t    //Most important statement which will initiate the Streaming Context\n",
    "\t    streamCtx.start();\n",
    "\t    //Wait till the execution is completed.\n",
    "\t    streamCtx.awaitTermination();  \n",
    "\t  \n",
    "\t  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The client application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "package chapter.two;\n",
    "\n",
    "import java.net.ServerSocket;\n",
    "import java.net.Socket;\n",
    "import java.io.*;\n",
    "\n",
    "public class ClientApp {\n",
    "\n",
    "\tpublic static void main(String[] args) {\n",
    "\t\ttry{\n",
    "\t\t\tSystem.out.println(\"Defining new Socket\");\n",
    "\t\t\tServerSocket soc = new ServerSocket(9087);\n",
    "\t\t\tSystem.out.println(\"Waiting for Incoming Connection\");\n",
    "\t\t\tSocket clientSocket = soc.accept();\n",
    "\n",
    "\t\t\tSystem.out.println(\"Connection Received\");\n",
    "\t\t\tOutputStream outputStream = clientSocket.getOutputStream();\n",
    "\t\t\t//Keep Reading the data in a Infinite loop and send it over to the Socket.\t\t\n",
    "\t\t\twhile(true){\n",
    "\t\t\t\tPrintWriter out =  new PrintWriter(outputStream, true);\n",
    "\t\t\t\tBufferedReader read = new BufferedReader(new InputStreamReader(System.in));\n",
    "\t\t\t\tSystem.out.println(\"Waiting for user to input some data\");\n",
    "\t\t\t\tString data = read.readLine();\n",
    "\t\t\t\tSystem.out.println(\"Data received and now writing it to Socket\");\n",
    "\t\t\t\tout.println(data);\n",
    "\t\t\t\t\n",
    "\t\t\t}\n",
    "\t\t\t\n",
    "\t\t}catch(Exception e ){\n",
    "\t\t\te.printStackTrace();\n",
    "\t\t}\n",
    "\n",
    "\n",
    "\t}\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packaging and deploying a Spark Streaming job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "java -classpath Spark-Example.jar chapter.two.ClientApp\n",
    "\n",
    "$SPARK_HOME/bin/spark-submit --class chapter.two.JavaFirstStreamingExample --master local Spark-Example.jar\n",
    "$SPARK_HOME/bin/spark-submit --class chapter.two.ScalaFirstStreamingExample --master local Spark-Example.jar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
