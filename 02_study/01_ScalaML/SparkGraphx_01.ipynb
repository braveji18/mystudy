{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01장 Getting Started with Spark and GraphX\n",
    "\n",
    "### Spark와 GraphX는 무엇인가?\n",
    "- 거대한 분산된 데이터셋을 처리하기 위한 클러스트 기반 컴퓨팅 플랫폼임.\n",
    "- 최적화된 병렬 컴퓨팅 엔진과  유연하고 단일한 API을 제공하여  빠르고 쉽게 데이터셋을 처리함.\n",
    "- Spark의 핵심 추상화 개념은 Resilient Distributed Dataset (RDD)을 기본 개념으로 함.\n",
    "- MapReduce framework을 확장하여 Spark Core API는 더욱 쉽게 데이터 분석 작업을 할 수 있음.\n",
    "- Spark는 머신러닝과 그래프 프로세싱과 같은 특별한 하이 레벨 Library을 제공함.\n",
    "- GraphX는 그래프 병렬 처리를 수행하는 Library임.\n",
    "\n",
    "### 이번장의 목표\n",
    "- Spark을 설치하기\n",
    "- Spark shell을 실험하고  Spark의 데이터 추상화에 대해서 알아보기\n",
    "- 기본 RDD과 graph 연산자들을  가지고 links들을 생성하고 탐색하기\n",
    "- SBT로 Spark 어플을 빌드 및 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01절. Downloading and installing Spark 1.4.1\n",
    "\n",
    "### Java Development Kit 7 (JDK) 설치\n",
    "- http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html \n",
    "\n",
    "### download the latest release of Spark\n",
    "- https://spark.apache.org/downloads.html\n",
    "- Pre-built for Hadoop 2.6 and later과 Direct Download을 선택\n",
    "- wget http://apache.mirror.cdnetworks.com/spark/spark-1.5.2/spark-1.5.2-bin-hadoop2.6.tgz\n",
    "- tar xvf spark-1.5.2-bin-hadoop2.6.tgz\n",
    "- ln -s spark-1.5.2-bin-hadoop2.6 spark\n",
    "- cd spark \n",
    "- ls \n",
    "\n",
    "### Spark의 내부 디렉토리 구조\n",
    "- core : sparkAPI와 핵심 구성요소의 소스코드가 포함됨.\n",
    "- bin : spark app과 spark shell을 실행시키기 위한 실행파일들이 포함됨.\n",
    "- graphx, mllib, sql, streaming : graph processing, machine learning, queries, and stream processing에 관련된 LIbrary\n",
    "- example : spark app의 예제와 데모를 포함.\n",
    "- conf : slave 노드와 기타 설정파일이 포함.\n",
    "\n",
    "### Spark 경로 설정\n",
    "- ~/.bashrc  or ~/.bash_profile\n",
    "```\n",
    "export SPARKHOME=\"/home/deepbio/app/spark/\"\n",
    "export SPARKSCALAEX=\"/home/deepbio/app/spark/examples/src/main/scala/org/apache/spark/examples/\"\n",
    "```\n",
    "\n",
    "- source ~/.bashrc or ~/.bash_profile\n",
    "- ls -al  $SPARKSCALAEX/graphx/LiveJournalPageRank.scala  # 경로 설정을 확인함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02절 Experimenting with the Spark shell\n",
    "\n",
    "- Scala 와 python을 위한 2가지 shell을 지원\n",
    "- GraphX는 대부분은 scala에서 제대로 동작함.\n",
    "- 실행방법\n",
    "\n",
    "```\n",
    "$SPARKHOME/bin/spark-shell  --driver-memory 2g\n",
    "\n",
    "cd $SPARKHOME\n",
    "./bin/spark-shell   \n",
    "```\n",
    "\n",
    "- 아래와 같은 에러가 발생하면 컴파일 안 된 소스만 받은 상태임.  prebuilt version 으로  다운로드 받음.\n",
    "```\n",
    "Failed to\n",
    "find Spark assembly in spark-1.4.1/assembly/target/\n",
    "scala-2.10. You need to build Spark before running\n",
    "this program,\n",
    "```\n",
    "\n",
    "- spark의 정상동작 확인\n",
    "```\n",
    "scala> sc\n",
    "scala> val myRDD = sc.parallelize(List(1,2,3,4,5))\n",
    "scala> sc.textFile(\"README.md\").filter(line => line contains \"Spark\").count()\n",
    "```\n",
    "\n",
    "- sc : Spark context is the point of entry to the Spark API\n",
    "- parallelize()함수를 호출해서 RDD 객체를 생성\n",
    "- README.md 파일을 로드해서 \"spark\" 라는 단어가 포함된 라인을 필터링하고  마지막으로 해당 라인의 수를 출력함.\n",
    "- spark의 기본적은 RDD 변환 및 action은 아래 URL을 참고함.\n",
    "```\n",
    "https://spark.apache.org/docs/latest/programming-guide.html\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03절 Getting started with GraphX\n",
    "\n",
    "#### 이번절의 목표\n",
    "- 1) 구체적인 예제를 통해서 Spark Core API와 GraphX API를 사용해서 graph를 생성하고 탐색하는 방법을 배움.\n",
    "- 2) graph processing을 하는데 중요한 스칼라 프로그램 특정을 알아봄.\n",
    "- 3) standalone Spark app을 개발하고 실행해봄.\n",
    "\n",
    "### Building a tiny social network\n",
    "- GraphX와 RDD 모듈을 import \n",
    "\n",
    "```\n",
    "scala> import org.apache.spark.graphx._\n",
    "scala> import org.apache.spark.rdd.RDD\n",
    "```\n",
    "\n",
    "### Loading the data\n",
    "- $SPARKHOME/data/ 디렉토리안의 people.csv와 links.csv 파일을 생성시킴.\n",
    "- people.csv의 내용\n",
    "```\n",
    "1,Alice,20\n",
    "2,Bob,18\n",
    "3,Charlie,30\n",
    "4,Dave,25\n",
    "5,Eve,30\n",
    "6,Faith,21\n",
    "7,George,34\n",
    "8,Harvey,47\n",
    "9,Ivy,21\n",
    "```\n",
    "\n",
    "- links.csv의 내용\n",
    "```\n",
    "1,2,friend\n",
    "1,3,sister\n",
    "3,2,boss\n",
    "2,4,brother\n",
    "4,5,client\n",
    "6,7,cousin\n",
    "7,9,coworker\n",
    "8,9,father\n",
    "1,9,friend\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "scala> val people = sc.textFile(\"./data/people.csv\")\n",
    "scala> val links = sc.textFile(\"./data/links.csv\")\n",
    "```\n",
    "\n",
    "- vertices와 edges의 알맞는 자료구조 변환하기 위해서 string 파싱이 필요함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The property graph\n",
    "- spark에서  graph을 정의하는 자료구조\n",
    "- 방향성 멀티 그래프\n",
    "    - 한 쌍의 vertices에는 여러개의 edges를 가질 수 있음.\n",
    "    - 각각의 edge는 방향성과 비방향성 관계를 가질 수 있음.\n",
    "```\n",
    "class Graph[VD, ED] {\n",
    "val vertices: VertexRDD[VD]\n",
    "val edges: EdgeRDD[ED,VD]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming RDDs to VertexRDD and EdgeRDD\n",
    "- graph를 구축하기 위한 3개의 단계\n",
    "    - 1) Persion 클래스 정의\n",
    "```\n",
    "case class Person(name: String, age: Int)\n",
    "```\n",
    "\n",
    "    - 2) people과 links 객체에 있는 csv 문자열을 파싱하고, Person 객체와 Edge 객체를 생성함.  각각의 결과는 RDD[(VertexId, Person)] 과  RDD[Edge[String]] 컬렉션안에 포함됨.\n",
    "\n",
    "```\n",
    "scala> val peopleRDD: RDD[(VertexId, Person)] = people map { line =>\n",
    "    val row = line split ','\n",
    "    (row(0).toInt, Person(row(1), row(2).toInt))\n",
    "}\n",
    "scala> type Connection = String\n",
    "scala> val linksRDD: RDD[Edge[Connection]] = links map {line =>\n",
    "val row = line split ','\n",
    "Edge(row(0).toInt, row(1).toInt, row(2))\n",
    "}\n",
    "\n",
    "```\n",
    "     - 3) Graph()의 생성자를 가지고 tinySocial 라는 이름을 갖는  social graph를 생성함.\n",
    "```\n",
    "scala> val tinySocial: Graph[Person, Connection] = Graph(peopleRDD, linksRDD)\n",
    "```    \n",
    "\n",
    "- Graph는 VertexRDD[VD]와 EdgeRDD[ED,VD]의 인스턴스를 멤버로 갖지만, 위의 코드에서는 RDD[(VertexId, Person)]와  RDD[Edge[Connection]] 을 가지고 생성자에 넘겨줌.\n",
    "- 이것이 가능한 이유는 => VertexRDD[VD]와 EdgeRDD[ED,VD]는 RDD[(VertexId, Person)] and RDD[Edge[Connection]]의 서브클래스로 동작함.    좀더 구체적인 이유는 나중에 설명함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing graph operations\n",
    "- vertices and edges을 내용물들을 확인해봄.\n",
    "```\n",
    "scala> tinySocial.vertices.collect()\n",
    "scala> tinySocial.edges.collect()\n",
    "```\n",
    "\n",
    "- professional 연결을 출력하기 위해서 profLinks 라는 리스트를 생성함.\n",
    "```\n",
    "scala> val profLinks: List[Connection] = List(\"coworker\", \"boss\", \"employee\",\"client\", \"supplier\")\n",
    "```\n",
    "\n",
    "```\n",
    "val profNetwork = tinySocial.edges.filter{ \n",
    "    case Edge(_,_,link) => profLinks.contains(link)\n",
    " }\n",
    "for {\n",
    "    Edge(src, dst, link) <- profNetwork.collect()\n",
    "    srcName = (peopleRDD.filter{case (id, person) => id == src} first)._2.name\n",
    "    dstName = (peopleRDD.filter{case (id, person) => id == dst} first)._2.name\n",
    "} println(srcName + \" is a \" + link + \" of \" + dstName)\n",
    "```\n",
    "\n",
    "- 위의 코드는 매우 비효율적임. \n",
    "- GraphX library 는 2가지 다른 view 방식을 제공\n",
    "     - 1) edges, vertices의 graph 또는 테이블 \n",
    "     - 2) triplets\n",
    "- 각각의 view에는 최적화된 많은 연산자들이 제공됨.\n",
    "```\n",
    "scala> tinySocial.subgraph(profLinks contains _.attr).\n",
    "    triplets.foreach(t => println(t.srcAttr.name + \" is a \" +\n",
    "    t.attr + \" of \" + t.dstAttr.name))\n",
    "```\n",
    "\n",
    "- subgraph()함수는 professional links을 filter하는데 사용하고, triplet view는 edges 와 vertices의 속성을 접근할때 사용함.\n",
    "- EdgeTriplet 은 3-tuple ((VertexId, Person), (VertexId, Person), Connection) 의 alias type임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04절. Building and submitting a standalone application\n",
    "\n",
    "### Writing and configuring a Spark program\n",
    "- simpleGraph.scala 파일을 만들고 아래 코드를 넣음.\n",
    "- http://www.packtpub.com 에서 가서 예제를 다운로드 받을 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "package com.github.giocode.graphxbook\n",
    "\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "object SimpleGraphApp {\n",
    "\tdef main(args: Array[String]){\n",
    "\t\t// Configure the program\n",
    "\t\tval conf = new SparkConf()\n",
    "\t\t\t\t\t.setAppName(\"Tiny Social\")\n",
    "\t\t\t\t\t.setMaster(\"local\")\n",
    "\t\t\t\t\t.set(\"spark.driver.memory\", \"2G\")\n",
    "\t\tval sc = new SparkContext(conf)\n",
    "\n",
    "\t\t// Load some data into RDDs\n",
    "\t\tval people = sc.textFile(\"../../data/people.csv\")\n",
    "\t\tval links = sc.textFile(\"../../data/links.csv\")\n",
    "\n",
    "\t\t// Parse the csv files into new RDDs\n",
    "\t\tcase class Person(name: String, age: Int)\n",
    "\t\ttype Connexion = String\n",
    "\t\tval peopleRDD: RDD[(VertexId, Person)] = people map { line => \n",
    "\t\t\tval row = line split ','\n",
    "\t\t\t(row(0).toInt, Person(row(1), row(2).toInt))\n",
    "\t\t}\n",
    "\t\tval linksRDD: RDD[Edge[Connexion]] = links map {line => \n",
    "\t\t\tval row = line split ','\n",
    "\t\t\tEdge(row(0).toInt, row(1).toInt, row(2))\n",
    "\t\t}\n",
    "\n",
    "\t\t// Create the social graph of people\n",
    "\t\tval tinySocial: Graph[Person, Connexion] = Graph(peopleRDD, linksRDD)\n",
    "\t\ttinySocial.cache()\n",
    "\t\ttinySocial.vertices.collect()\n",
    "\t\ttinySocial.edges.collect()\n",
    "\n",
    "\t\t// Extract links between coworkers and print their professional relationship\n",
    "\t\tval profLinks: List[Connexion] = List(\"coworker\", \"boss\", \"employee\",\"client\", \"supplier\")\n",
    "\t\ttinySocial.subgraph(profLinks contains _.attr).\n",
    "\t\t\t\t   triplets.foreach(t => println(t.srcAttr.name + \" is a \" + t.attr + \" of \" + t.dstAttr.name))\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the program with the Scala Build Tool \n",
    "\n",
    "- http://www.scala-sbt.org/0.13/tutorial/index.html 에서 SBT 설치방법, 수동설치보다는 package manager로 설치를 추천함.\n",
    "- 우분투에서 설치 방법\n",
    "```\n",
    "echo \"deb https://dl.bintray.com/sbt/debian /\" | sudo tee -a /etc/apt/sources.list.d/sbt.list\n",
    "sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 642AC823\n",
    "sudo apt-get update\n",
    "sudo apt-get install sbt\n",
    "```\n",
    "\n",
    "- CentOS에서 설치 방법\n",
    "```\n",
    "curl https://bintray.com/sbt/rpm/rpm | sudo tee /etc/yum.repos.d/bintray-sbt-rpm.repo\n",
    "sudo yum install sbt\n",
    "```\n",
    "\n",
    "- build.sbt 파일 생성\n",
    "```\n",
    "name := \"Simple Project\"\n",
    "version := \"1.0\"\n",
    "scalaVersion := \"2.10.4\"\n",
    "libraryDependencies ++= Seq(\n",
    "    \"org.apache.spark\" %% \"spark-core\" % \"1.4.1\",\n",
    "    \"org.apache.spark\" %% \"spark-graphx\" % \"1.4.1\"\n",
    ")\n",
    "resolvers += \"Akka Repository\" at \"http://repo.akka.io/releases/\"\n",
    "```\n",
    "\n",
    "- 빌드  : \n",
    "```\n",
    "$ sbt package\n",
    "```\n",
    "\n",
    "- simple-project_2.10-1.0.jar 라는 jar 생성됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying and running with spark-submit\n",
    "- 아래와 같이 실행함.\n",
    "\n",
    "```\n",
    "cd target/scala-2.10/\n",
    "\n",
    "../../bin/spark-submit --class \\\n",
    "  com.github.giocode.graphxbook.SimpleGraphApp \\\n",
    " ./simple-project_2.10-1.0.jar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
